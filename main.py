import torch
import torch.nn as nn
import torch.nn.functional as F
import os
from typing import List, Optional
from dataclasses import dataclass
import transformers

# ==============================================
# å…¨å±€é…ç½®ï¼ˆé€šç”¨æ‰€æœ‰å¤§æ¨¡å‹ï¼‰
# ==============================================
@dataclass
class KnowledgeChipConfig:
    hidden_dim: int = None    # è‡ªåŠ¨ä»æ¨¡å‹è·å–
    top_k: int = 1
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    chip_dir: str = "./knowledge_chips"
    use_gate_fusion: bool = True

os.makedirs(KnowledgeChipConfig.chip_dir, exist_ok=True)

# ==============================================
# ğŸ§© çŸ¥è¯†èŠ¯ç‰‡ï¼ˆç‹¬ç«‹è®­ç»ƒã€ç‹¬ç«‹ä¿å­˜ï¼‰
# ==============================================
class KnowledgeChip(nn.Module):
    def __init__(self, hidden_dim, chip_id: str, desc: str = ""):
        super().__init__()
        self.chip_id = chip_id
        self.desc = desc
        self.hidden_dim = hidden_dim

        self.k_net = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, hidden_dim)
        )

    def forward(self, h):
        return self.k_net(h)

    def save(self):
        path = os.path.join(KnowledgeChipConfig.chip_dir, f"{self.chip_id}.pt")
        torch.save({
            "state_dict": self.state_dict(),
            "chip_id": self.chip_id,
            "desc": self.desc,
            "hidden_dim": self.hidden_dim
        }, path)

    @staticmethod
    def load(chip_id):
        path = os.path.join(KnowledgeChipConfig.chip_dir, f"{ch chip_id}.pt")
        ckpt = torch.load(path, map_location=KnowledgeChipConfig.device)
        chip = KnowledgeChip(ckpt["hidden_dim"], ckpt["chip_id"], ckpt["desc"])
        chip.load_state_dict(ckpt["state_dict"])
        return chip

# ==============================================
# ğŸš¦ è·¯ç”±å™¨ + èåˆå±‚ï¼ˆé€šç”¨ç»“æ„ï¼‰
# ==============================================
class KnowledgeRouter(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.q_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)

    @torch.no_grad()
    def score(self, h, chips):
        query = self.q_proj(h)
        scores = []
        for chip in chips:
            k_vec = chip(h)
            sim = F.cosine_similarity(query, k_vec, dim=-1).mean(dim=-1)
            scores.append(sim)
        return torch.stack(scores, dim=-1)

class KnowledgeFusion(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.gate = nn.Sequential(nn.Linear(hidden_dim*2, hidden_dim), nn.Sigmoid())

    def forward(self, h, knowledge):
        g = self.gate(torch.cat([h, knowledge], dim=-1))
        return h + g * knowledge

# ==============================================
# ğŸ­ é€šç”¨çŸ¥è¯†èŠ¯ç‰‡å¼•æ“ï¼ˆæ ¸å¿ƒï¼é€‚é…æ‰€æœ‰å¤§æ¨¡å‹ï¼‰
# ==============================================
class UniversalKnowledgeEngine:
    def __init__(self, config: KnowledgeChipConfig):
        self.config = config
        self.device = config.device
        self.chips: List[KnowledgeChip] = []
        self.router = None
        self.fusion = None

    def init_with_model(self, model):
        """è‡ªåŠ¨è¯»å–æ¨¡å‹ç»´åº¦ï¼Œé€‚é…æ‰€æœ‰å¤§æ¨¡å‹"""
        hidden_dim = model.config.hidden_size
        self.config.hidden_dim = hidden_dim
        self.router = KnowledgeRouter(hidden_dim).to(self.device).eval()
        self.fusion = KnowledgeFusion(hidden_dim).to(self.device).eval()
        print(f"âœ… è‡ªåŠ¨é€‚é…æ¨¡å‹: hidden_dim = {hidden_dim}")

    def add_chip(self, chip: KnowledgeChip):
        chip.eval().to(self.device)
        self.chips.append(chip)
        print(f"ğŸ”Œ åŠ è½½çŸ¥è¯†èŠ¯ç‰‡: {chip.chip_id} | {chip.desc}")

    def remove_chip(self, chip_id):
        self.chips = [c for c in self.chips if c.chip_id != chip_id]

    @torch.no_grad()
    def enhance_hidden(self, hidden_states):
        if not self.chips:
            return hidden_states

        scores = self.router.score(hidden_states, self.chips)
        top_vals, top_idx = torch.topk(scores, k=self.config.top_k)
        weights = top_vals.softmax(dim=-1)

        B, L, D = hidden_states.shape
        knowledge = torch.zeros_like(hidden_states)
        for b in range(B):
            ks = 0
            for i, w in zip(top_idx[b], weights[b]):
                ks += w * self.chips[i](hidden_states[b:b+1])
            knowledge[b] = ks

        return self.fusion(hidden_states, knowledge)

# ==============================================
# ğŸ”— ä¸‡èƒ½ Hookï¼šè‡ªåŠ¨æ³¨å…¥ FFN å‰ï¼ˆLlama/Qwen/ChatGLM é€šç”¨ï¼‰
# ==============================================
def apply_knowledge_chip_hook(model, engine: UniversalKnowledgeEngine):
    """
    è‡ªåŠ¨ç»™æ‰€æœ‰å±‚ FFN å‰æ³¨å…¥çŸ¥è¯†
    æ”¯æŒï¼šLlama, Qwen, ChatGLM, Baichuan, DeepSeek
    """
    def ffnhook(module, input, output):
        hidden_states = input[0]
        enhanced = engine.enhance_hidden(hidden_states)
        return (enhanced,) + input[1:]

    # è‡ªåŠ¨æ‰¾åˆ°æ‰€æœ‰ FFN å¹¶æ³¨å†Œ hook
    for name, module in model.named_modules():
        if any(kw in name.lower() for kw in ["mlp", "ffn", "feedforward"]):
            module.register_forward_hook(ffnhook)
            print(f"ğŸ¯ Hook æ³¨å…¥: {name}")

# ==============================================
# ğŸš€ éƒ¨ç½²æ¼”ç¤ºï¼šä¸€é”®åŠ è½½ä»»æ„å¤§æ¨¡å‹ + çŸ¥è¯†èŠ¯ç‰‡
# ==============================================
if __name__ == "__main__":
    # -------------------
    # 1. åŠ è½½ä»»æ„å¤§æ¨¡å‹
    # -------------------
    model_name = "Qwen/Qwen-1.5-0.5B-Chat"  # å¯æ›¿æ¢ï¼š
    # model_name = "meta-llama/Llama-2-7b-chat-hf"
    # model_name = "THUDM/chatglm3-6b"
    # model_name = "baichuan-inc/Baichuan2-7B-Chat"

    model = transformers.AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)

    # -------------------
    # 2. åˆå§‹åŒ–çŸ¥è¯†èŠ¯ç‰‡å¼•æ“
    # -------------------
    cfg = KnowledgeChipConfig()
    engine = UniversalKnowledgeEngine(cfg)
    engine.init_with_model(model)

    # -------------------
    # 3. åŠ è½½/åˆ›å»ºèŠ¯ç‰‡
    # -------------------
    chip1 = KnowledgeChip(model.config.hidden_size, "math_v2", "æ•°å­¦çŸ¥è¯†")
    chip2 = KnowledgeChip(model.config.hidden_size, "law_v1", "æ³•å¾‹çŸ¥è¯†")
    chip1.save()
    chip2.save()

    engine.add_chip(chip1)
    engine.add_chip(chip2)

    # -------------------
    # 4. æ³¨å…¥ Hookï¼ˆä¸‡èƒ½é€‚é…ï¼‰
    # -------------------
    apply_knowledge_chip_hook(model, engine)

    # -------------------
    # 5. æ¨ç†æµ‹è¯•
    # -------------------
    text = "è¯·è§£é‡Šä¸€ä¸‹ä¸‰è§’å½¢å†…è§’å’Œ"
    inputs = tokenizer([text], return_tensors="pt").to(cfg.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=256,
        temperature=0.3
    )
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))
    print("\nâœ… é€šç”¨å¤§æ¨¡å‹ + çŸ¥è¯†èŠ¯ç‰‡ éƒ¨ç½²æˆåŠŸï¼")